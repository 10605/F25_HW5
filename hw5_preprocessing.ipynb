{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, StructType, StructField\n",
    "from io import BytesIO\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "try:\n",
    "    if spark is not None:\n",
    "        print(\"Spark already defined\")\n",
    "except NameError:\n",
    "    print(\"Starting Spark\")\n",
    "    spark = SparkSession.builder.appName(\"MyJob\").getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1042c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N = 256\n",
    "MAX_VAL = (2**32) - 1\n",
    "PRIME = 4294967311\n",
    "PRIME_NP = np.uint64(PRIME)\n",
    "MIN_TEXT_LENGTH = 300\n",
    "\n",
    "# Fixed random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "A_COEFFS = np.random.randint(0, MAX_VAL + 1, size=N, dtype=np.uint64)\n",
    "B_COEFFS = np.random.randint(0, MAX_VAL + 1, size=N, dtype=np.uint64)\n",
    "\n",
    "# Binary file writing function\n",
    "HEADERS_INFO = {\n",
    "    \"gpt-2\": {\n",
    "        \"magic\": 20240520,\n",
    "        \"version\": 1,\n",
    "        \"token_dtype\": np.uint16,\n",
    "    },\n",
    "    \"llama-3\": {\n",
    "        \"magic\": 20240801,\n",
    "        \"version\": 7,\n",
    "        \"token_dtype\": np.uint32,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_numpy(s):\n",
    "    \"\"\"Calculate MinHash signature for a set.\"\"\"\n",
    "    signature = np.full(N, np.iinfo(np.uint64).max, dtype=np.uint64)\n",
    "\n",
    "    for val in s:\n",
    "        val_int = hash(val) % PRIME if not isinstance(val, (int, np.integer)) else val\n",
    "        val_np = np.uint64(val_int)\n",
    "        current_hashes = (A_COEFFS * val_np + B_COEFFS) % PRIME_NP\n",
    "        signature = np.minimum(signature, current_hashes)\n",
    "\n",
    "    return signature.tolist()\n",
    "\n",
    "\n",
    "def bucket_range(hash_arr, start, end):\n",
    "    \"\"\"Extract bucket key from hash array.\"\"\"\n",
    "    if hash_arr is None or len(hash_arr) <= start:\n",
    "        return None\n",
    "    actual_end = min(end, len(hash_arr))\n",
    "    return \"-\".join(str(x) for x in hash_arr[start:actual_end])\n",
    "\n",
    "\n",
    "def html_to_text(html):\n",
    "    \"\"\"Extract text from HTML, removing scripts and styles.\"\"\"\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    if isinstance(html, (bytearray, bytes)):\n",
    "        html = html.decode('utf-8', errors='ignore')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text if text else None\n",
    "\n",
    "\n",
    "def html_to_text_simple(html_content):\n",
    "    \"\"\"Convert HTML to plain text for UDF.\"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(html_content, (bytearray, bytes)):\n",
    "        html_content = html_content.decode('utf-8', errors='ignore')\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        return soup.get_text(strip=True)\n",
    "    except Exception:\n",
    "        return str(html_content) if html_content else \"\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text using GPT-2 tokenizer.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    eot = enc._special_tokens['<|endoftext|>']\n",
    "\n",
    "    if not text:\n",
    "        return [eot]\n",
    "\n",
    "    try:\n",
    "        tokens = [eot]\n",
    "        tokens.extend(enc.encode_ordinary(text))\n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all()\n",
    "        return tokens_np.astype(int).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing: {e}\")\n",
    "        return [eot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_partition_to_s3(partition_id, rows, s3_bucket, s3_prefix, model_desc=\"gpt-2\"):\n",
    "    \"\"\"Write tokens from partition to binary buffer and upload to S3.\"\"\"\n",
    "    tokens = []\n",
    "    for row in rows:\n",
    "        if row.tokens:\n",
    "            tokens.extend(row.tokens)\n",
    "    \n",
    "    if not tokens:\n",
    "        return iter([])\n",
    "    \n",
    "    # Build binary in memory\n",
    "    info = HEADERS_INFO[model_desc]\n",
    "    \n",
    "    # Construct header\n",
    "    header = np.zeros(256, dtype=np.int32)\n",
    "    header[0] = info[\"magic\"]\n",
    "    header[1] = info[\"version\"]\n",
    "    header[2] = len(tokens)\n",
    "    \n",
    "    # Construct token array\n",
    "    toks_np = np.array(tokens, dtype=info[\"token_dtype\"])\n",
    "    \n",
    "    # Write to in-memory buffer\n",
    "    buffer = BytesIO()\n",
    "    buffer.write(header.tobytes())\n",
    "    buffer.write(toks_np.tobytes())\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_key = f\"{s3_prefix}/tokens_{partition_id:06d}.bin\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_fileobj(buffer, s3_bucket, s3_key)\n",
    "    \n",
    "    num_bytes = (256 * 4) + (len(tokens) * toks_np.itemsize)\n",
    "    print(f\"Partition {partition_id}: uploaded {len(tokens):,} tokens ({num_bytes:,} bytes) to s3://{s3_bucket}/{s3_key}\")\n",
    "    \n",
    "    return iter([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d77b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDFs\n",
    "html_to_text_udf = F.udf(html_to_text_simple, StringType())\n",
    "tokenize_udf = F.udf(tokenize, ArrayType(IntegerType()))\n",
    "bucket_range_udf = F.udf(bucket_range, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77becb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: select your dataset.\n",
    "dataset = \"1k\" # \"10k\" \"100k\" \"1m\" \"5m\"\n",
    "\n",
    "if dataset == \"1k\":\n",
    "    input_path = \"s3://10605-f25-hw5-subset-1000/*.parquet\"\n",
    "\n",
    "elif dataset == \"10k\":\n",
    "    input_path = \"s3://10605-f25-hw5-subset-10000/*.parquet\"\n",
    "    \n",
    "else:\n",
    "    input_path = None # TODO: Put your s3 bucket here!!\n",
    "\n",
    "\n",
    "# TODO: Fill this in\n",
    "output_path = None # TODO: Put your s3 bucket output path here!!\n",
    "\n",
    "\n",
    "model_desc=\"gpt-2\",\n",
    "tokens_per_file=10**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "print(f\"Reading data from {input_path}\")\n",
    "df = spark.read.parquet(input_path)\n",
    "\n",
    "print(f\"Initial row count: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "print(\"Shuffling data...\")\n",
    "df = df.orderBy(F.rand())\n",
    "\n",
    "# Extract and tokenize text\n",
    "print(\"Extracting text from HTML...\")\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"data\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "text_rdd = (df.rdd\n",
    "            .map(lambda x: (x[0], html_to_text(x[1])))\n",
    "            .filter(lambda x: x[1] is not None))\n",
    "\n",
    "token_rdd = text_rdd.map(lambda x: (x[0], tokenize(x[1])))\n",
    "df_tokens = token_rdd.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and create sets\n",
    "print(\"Filtering and creating token sets...\")\n",
    "df_split = (df_tokens\n",
    "            .filter(F.size(\"data\") >= MIN_TEXT_LENGTH))\n",
    "\n",
    "test_rdd = df_split.rdd.map(lambda x: (x[0], set(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MinHash\n",
    "print(\"Calculating MinHash signatures...\")\n",
    "minhash_rdd = test_rdd.map(lambda x: (x[0], minhash_numpy(x[1])))\n",
    "minhash_df = minhash_rdd.toDF([\"item_id\", \"hash_sequence\"]).cache()\n",
    "\n",
    "# Create buckets for LSH\n",
    "print(\"Creating LSH buckets...\")\n",
    "bucket_ranges = [(i, i + 32) for i in range(0, 256, 32)]\n",
    "ranges_df = spark.createDataFrame(bucket_ranges, [\"bucket_start\", \"bucket_end\"])\n",
    "\n",
    "# Find duplicates using LSH\n",
    "print(\"Finding duplicates...\")\n",
    "results_df = (minhash_df\n",
    "                .crossJoin(F.broadcast(ranges_df))\n",
    "                .withColumn(\"bucket_key\", \n",
    "                            bucket_range_udf(F.col(\"hash_sequence\"), \n",
    "                                            F.col(\"bucket_start\"), \n",
    "                                            F.col(\"bucket_end\")))\n",
    "                .filter(F.col(\"bucket_key\").isNotNull())\n",
    "                .groupBy(\"bucket_key\")\n",
    "                .agg(F.collect_list(\"item_id\").alias(\"items_in_bucket\"))\n",
    "                .filter(F.size(\"items_in_bucket\") > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2750db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate buckets\n",
    "# TODO: Using the clustered items, deduplicate the original dataframe\n",
    "\n",
    "\n",
    "result_df = df_deduped.select(\"id\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"1k\" or dataset == \"10k\":\n",
    "    # WARNING!!! DO NOT RUN COALESCE ON YOUR FULL DATASET!!!\n",
    "    # YOU WILL CRASH YOUR CLUSTER AND NEED TO REPEAT THE ENTIRE PROCESS\n",
    "    result_df.select(\"id\", \"data\").coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "else:\n",
    "    result_df.select(\"data\").write.mode(\"overwrite\").parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
